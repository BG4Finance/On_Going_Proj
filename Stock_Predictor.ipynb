{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STOCCA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BG4Finance/On_Going_Proj/blob/master/Stock_Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rIFLJozsGgn",
        "colab_type": "text"
      },
      "source": [
        "# Stocca setting UP\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4UTwEQrokun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install yahoo_fin\n",
        "! pip install requests_html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHuNxuQdnKFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as dt\n",
        "import tensorflow as tf\n",
        "from yahoo_fin.stock_info import get_analysts_info, get_balance_sheet, get_live_price, get_data\n",
        "import pandas_datareader as web\n",
        "import requests_html\n",
        "import csv\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um2IyhBrtGUP",
        "colab_type": "text"
      },
      "source": [
        "# Data Gathering\n",
        "S&P companies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvkQ2UUx_6Bt",
        "colab_type": "code",
        "outputId": "bfb29c29-e055-46cb-e6a2-c0437c99a3b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "today = str(dt.date.today())\n",
        "print(today)\n",
        "start = \"2018-01-01\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-11-25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eExZaMe-iR82",
        "outputId": "ec46afde-0bb7-4d47-8e4b-e28aa2627aef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tickers = pd.read_csv('/content/complete.csv')\n",
        "len(tickers)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "505"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LoTHpTJ0fn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#FIrst return vector\n",
        "ret_cost = web.get_data_yahoo(tickers['Symbol'][0], start = start, end = today)\n",
        "ret_cost = np.array(ret_cost['Adj Close'].ffill().pct_change())\n",
        "ret_cost = ret_cost.reshape(ret_cost.shape[0],1)\n",
        "#Second return vector\n",
        "vec = web.get_data_yahoo(tickers['Symbol'][1], start = start, end = today) \n",
        "vec = np.array(vec['Adj Close'].ffill().pct_change())\n",
        "vec = vec.reshape(vec.shape[0],1)\n",
        "#Matrix building\n",
        "ret_cost = np.hstack((ret_cost,vec))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkgFGcHMdWJz",
        "colab_type": "code",
        "outputId": "677a9d89-f20a-4937-e57e-90628faf0e10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "ret_cost = web.get_data_yahoo(tickers['Symbol'][0], start = start, end = today)\n",
        "ret_cost = np.array(ret_cost['Adj Close'].ffill().pct_change())\n",
        "ret_cost = ret_cost.reshape(ret_cost.shape[0],1)\n",
        "ejected = 0\n",
        "for var in range(1,len(tickers)) :\n",
        "  try:\n",
        "    vec = web.get_data_yahoo(tickers['Symbol'][var], start = start, end = today) \n",
        "    vec = np.array(vec['Adj Close'].ffill().pct_change())\n",
        "    vec = vec.reshape(vec.shape[0],1)\n",
        "    if vec.shape[0]==ret_cost.shape[0]:\n",
        "      ret_cost = np.hstack((ret_cost,vec), )\n",
        "      print(vec[1])\n",
        "    else:\n",
        "      print(\"padding avoided, SKIP at\", var)\n",
        "      ejected = ejected + 1\n",
        "  except:\n",
        "    print('EJECT at:', var)\n",
        "    ejected = ejected + 1\n",
        "  print(ret_cost.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0060133]\n",
            "(478, 2)\n",
            "[0.00221116]\n",
            "(478, 3)\n",
            "[0.0156486]\n",
            "(478, 4)\n",
            "[0.00461524]\n",
            "(478, 5)\n",
            "[0.01554967]\n",
            "(478, 6)\n",
            "[-0.01206844]\n",
            "(478, 7)\n",
            "[0.0187957]\n",
            "(478, 8)\n",
            "[0.00904894]\n",
            "(478, 9)\n",
            "[0.05191263]\n",
            "(478, 10)\n",
            "[-0.0009192]\n",
            "(478, 11)\n",
            "EJECT at: 11\n",
            "(478, 11)\n",
            "[-0.00453116]\n",
            "(478, 12)\n",
            "[0.00295552]\n",
            "(478, 13)\n",
            "[0.02544373]\n",
            "(478, 14)\n",
            "[0.0054376]\n",
            "(478, 15)\n",
            "[0.00579629]\n",
            "(478, 16)\n",
            "[-0.00466593]\n",
            "(478, 17)\n",
            "[0.00197664]\n",
            "(478, 18)\n",
            "[-0.00469476]\n",
            "(478, 19)\n",
            "[0.00725509]\n",
            "(478, 20)\n",
            "[0.02929393]\n",
            "(478, 21)\n",
            "[0.00425476]\n",
            "(478, 22)\n",
            "[-0.00105684]\n",
            "(478, 23)\n",
            "[0.01856416]\n",
            "(478, 24)\n",
            "[-0.00878648]\n",
            "(478, 25)\n",
            "[0.00058876]\n",
            "(478, 26)\n",
            "[0.01706102]\n",
            "(478, 27)\n",
            "[0.01641313]\n",
            "(478, 28)\n",
            "EJECT at: 29\n",
            "(478, 28)\n",
            "[0.01277528]\n",
            "(478, 29)\n",
            "[-0.00513782]\n",
            "(478, 30)\n",
            "[-0.01226657]\n",
            "(478, 31)\n",
            "[-0.00842444]\n",
            "(478, 32)\n",
            "[0.00616539]\n",
            "(478, 33)\n",
            "[0.01109236]\n",
            "(478, 34)\n",
            "[0.00389708]\n",
            "(478, 35)\n",
            "[-0.00707245]\n",
            "(478, 36)\n",
            "EJECT at: 38\n",
            "(478, 36)\n",
            "[0.00372186]\n",
            "(478, 37)\n",
            "[0.00879722]\n",
            "(478, 38)\n",
            "[0.01887004]\n",
            "(478, 39)\n",
            "[0.01298401]\n",
            "(478, 40)\n",
            "EJECT at: 43\n",
            "(478, 40)\n",
            "[0.01240588]\n",
            "(478, 41)\n",
            "EJECT at: 45\n",
            "(478, 41)\n",
            "[0.01932006]\n",
            "(478, 42)\n",
            "[0.01393813]\n",
            "(478, 43)\n",
            "[0.01597094]\n",
            "(478, 44)\n",
            "[0.02325047]\n",
            "(478, 45)\n",
            "[0.00138078]\n",
            "(478, 46)\n",
            "[-0.00017413]\n",
            "(478, 47)\n",
            "[0.01772245]\n",
            "(478, 48)\n",
            "[0.01660655]\n",
            "(478, 49)\n",
            "[-0.00773256]\n",
            "(478, 50)\n",
            "[0.00792506]\n",
            "(478, 51)\n",
            "[0.00755996]\n",
            "(478, 52)\n",
            "[0.00282178]\n",
            "(478, 53)\n",
            "[-0.02309269]\n",
            "(478, 54)\n",
            "[0.02109778]\n",
            "(478, 55)\n",
            "[0.01086302]\n",
            "(478, 56)\n",
            "[0.01750077]\n",
            "(478, 57)\n",
            "[0.003103]\n",
            "(478, 58)\n",
            "[0.0028527]\n",
            "(478, 59)\n",
            "padding avoided, SKIP at 64\n",
            "(478, 59)\n",
            "[-0.00258877]\n",
            "(478, 60)\n",
            "[-0.00334449]\n",
            "(478, 61)\n",
            "[0.01607878]\n",
            "(478, 62)\n",
            "[0.01081726]\n",
            "(478, 63)\n",
            "[0.00986966]\n",
            "(478, 64)\n",
            "EJECT at: 70\n",
            "(478, 64)\n",
            "[-0.01098898]\n",
            "(478, 65)\n",
            "[0.01699731]\n",
            "(478, 66)\n",
            "[0.01055075]\n",
            "(478, 67)\n",
            "[-0.00411067]\n",
            "(478, 68)\n",
            "[0.00323407]\n",
            "(478, 69)\n",
            "[0.02208154]\n",
            "(478, 70)\n",
            "[0.00502128]\n",
            "(478, 71)\n",
            "[0.00054141]\n",
            "(478, 72)\n",
            "[0.0110759]\n",
            "(478, 73)\n",
            "[0.01456564]\n",
            "(478, 74)\n",
            "[0.00081549]\n",
            "(478, 75)\n",
            "[0.01093602]\n",
            "(478, 76)\n",
            "EJECT at: 83\n",
            "(478, 76)\n",
            "[-0.00278589]\n",
            "(478, 77)\n",
            "padding avoided, SKIP at 85\n",
            "(478, 77)\n",
            "[0.00449837]\n",
            "(478, 78)\n",
            "[0.02092749]\n",
            "(478, 79)\n",
            "[-0.01413807]\n",
            "(478, 80)\n",
            "[-0.00170665]\n",
            "(478, 81)\n",
            "[-0.00585151]\n",
            "(478, 82)\n",
            "[0.03366276]\n",
            "(478, 83)\n",
            "[0.00224724]\n",
            "(478, 84)\n",
            "[0.00152832]\n",
            "(478, 85)\n",
            "[0.00659877]\n",
            "(478, 86)\n",
            "[0.00844361]\n",
            "(478, 87)\n",
            "[0.00050713]\n",
            "(478, 88)\n",
            "padding avoided, SKIP at 97\n",
            "(478, 88)\n",
            "[0.01793551]\n",
            "(478, 89)\n",
            "[-0.00214129]\n",
            "(478, 90)\n",
            "[-0.01895455]\n",
            "(478, 91)\n",
            "[0.00729181]\n",
            "(478, 92)\n",
            "[0.00023177]\n",
            "(478, 93)\n",
            "[0.00155511]\n",
            "(478, 94)\n",
            "[0.01163423]\n",
            "(478, 95)\n",
            "[0.00728952]\n",
            "(478, 96)\n",
            "[0.05478746]\n",
            "(478, 97)\n",
            "[0.00787066]\n",
            "(478, 98)\n",
            "[-0.0064739]\n",
            "(478, 99)\n",
            "[0.01678521]\n",
            "(478, 100)\n",
            "[0.00549747]\n",
            "(478, 101)\n",
            "[-0.00396851]\n",
            "(478, 102)\n",
            "[0.0100095]\n",
            "(478, 103)\n",
            "[0.00797739]\n",
            "(478, 104)\n",
            "[0.00309308]\n",
            "(478, 105)\n",
            "[0.01148078]\n",
            "(478, 106)\n",
            "[0.01082419]\n",
            "(478, 107)\n",
            "[0.01602316]\n",
            "(478, 108)\n",
            "[-0.00617696]\n",
            "(478, 109)\n",
            "[-0.00219602]\n",
            "(478, 110)\n",
            "[0.00845661]\n",
            "(478, 111)\n",
            "[-0.00385959]\n",
            "(478, 112)\n",
            "[-0.01607011]\n",
            "(478, 113)\n",
            "[-0.00034652]\n",
            "(478, 114)\n",
            "[-0.00506406]\n",
            "(478, 115)\n",
            "[0.00454243]\n",
            "(478, 116)\n",
            "[0.01843826]\n",
            "(478, 117)\n",
            "[-0.00945196]\n",
            "(478, 118)\n",
            "[0.00414896]\n",
            "(478, 119)\n",
            "[0.02452483]\n",
            "(478, 120)\n",
            "[0.01200078]\n",
            "(478, 121)\n",
            "[-0.00553321]\n",
            "(478, 122)\n",
            "[-0.00422293]\n",
            "(478, 123)\n",
            "padding avoided, SKIP at 133\n",
            "(478, 123)\n",
            "[0.01260208]\n",
            "(478, 124)\n",
            "[0.01101312]\n",
            "(478, 125)\n",
            "[-0.00435261]\n",
            "(478, 126)\n",
            "[0.03525262]\n",
            "(478, 127)\n",
            "[0.01330581]\n",
            "(478, 128)\n",
            "[0.02304833]\n",
            "(478, 129)\n",
            "[0.00510067]\n",
            "(478, 130)\n",
            "[0.00455674]\n",
            "(478, 131)\n",
            "[-0.01850538]\n",
            "(478, 132)\n",
            "[-0.00342569]\n",
            "(478, 133)\n",
            "[0.01160304]\n",
            "(478, 134)\n",
            "[0.00914257]\n",
            "(478, 135)\n",
            "[0.00741112]\n",
            "(478, 136)\n",
            "[-0.02120293]\n",
            "(478, 137)\n",
            "[-0.016033]\n",
            "(478, 138)\n",
            "[-0.01673307]\n",
            "(478, 139)\n",
            "[-0.00074422]\n",
            "(478, 140)\n",
            "[-0.00616437]\n",
            "(478, 141)\n",
            "[-0.03849004]\n",
            "(478, 142)\n",
            "[0.00806385]\n",
            "(478, 143)\n",
            "EJECT at: 154\n",
            "(478, 143)\n",
            "EJECT at: 155\n",
            "(478, 143)\n",
            "[-0.0032274]\n",
            "(478, 144)\n",
            "[-0.00312373]\n",
            "(478, 145)\n",
            "[-0.00297502]\n",
            "(478, 146)\n",
            "[0.01893748]\n",
            "(478, 147)\n",
            "[0.01605786]\n",
            "(478, 148)\n",
            "[0.01145639]\n",
            "(478, 149)\n",
            "[0.03525246]\n",
            "(478, 150)\n",
            "[0.0304782]\n",
            "(478, 151)\n",
            "[0.0064686]\n",
            "(478, 152)\n",
            "[-0.00790514]\n",
            "(478, 153)\n",
            "[0.02697611]\n",
            "(478, 154)\n",
            "[0.00274101]\n",
            "(478, 155)\n",
            "[0.00918845]\n",
            "(478, 156)\n",
            "[-0.01101699]\n",
            "(478, 157)\n",
            "padding avoided, SKIP at 170\n",
            "(478, 157)\n",
            "[0.01491312]\n",
            "(478, 158)\n",
            "[0.00528844]\n",
            "(478, 159)\n",
            "[0.00518651]\n",
            "(478, 160)\n",
            "[0.00460925]\n",
            "(478, 161)\n",
            "[0.00519611]\n",
            "(478, 162)\n",
            "[0.01024645]\n",
            "(478, 163)\n",
            "[0.0159103]\n",
            "(478, 164)\n",
            "[0.01524459]\n",
            "(478, 165)\n",
            "[-0.0088677]\n",
            "(478, 166)\n",
            "[-0.01338723]\n",
            "(478, 167)\n",
            "[0.03088447]\n",
            "(478, 168)\n",
            "[0.00138491]\n",
            "(478, 169)\n",
            "EJECT at: 183\n",
            "(478, 169)\n",
            "[0.00210086]\n",
            "(478, 170)\n",
            "[0.01963998]\n",
            "(478, 171)\n",
            "[0.01101215]\n",
            "(478, 172)\n",
            "[0.01791423]\n",
            "(478, 173)\n",
            "[0.0003667]\n",
            "(478, 174)\n",
            "[-0.0037382]\n",
            "(478, 175)\n",
            "[0.01249993]\n",
            "(478, 176)\n",
            "[0.00586983]\n",
            "(478, 177)\n",
            "[0.00295758]\n",
            "(478, 178)\n",
            "[-0.01427651]\n",
            "(478, 179)\n",
            "[0.00813193]\n",
            "(478, 180)\n",
            "[0.00785396]\n",
            "(478, 181)\n",
            "[-0.00305895]\n",
            "(478, 182)\n",
            "[0.01471422]\n",
            "(478, 183)\n",
            "[0.01284837]\n",
            "(478, 184)\n",
            "[-0.00041781]\n",
            "(478, 185)\n",
            "[0.00789916]\n",
            "(478, 186)\n",
            "[0.0005527]\n",
            "(478, 187)\n",
            "[0.01178358]\n",
            "(478, 188)\n",
            "[0.0046243]\n",
            "(478, 189)\n",
            "[-0.01517454]\n",
            "(478, 190)\n",
            "[-0.02066726]\n",
            "(478, 191)\n",
            "[0.01226061]\n",
            "(478, 192)\n",
            "[0.0027233]\n",
            "(478, 193)\n",
            "[0.00623476]\n",
            "(478, 194)\n",
            "[0.00945499]\n",
            "(478, 195)\n",
            "EJECT at: 210\n",
            "(478, 195)\n",
            "[-0.00525052]\n",
            "(478, 196)\n",
            "[0.02440186]\n",
            "(478, 197)\n",
            "[0.00745516]\n",
            "(478, 198)\n",
            "[0.00755746]\n",
            "(478, 199)\n",
            "[0.02159501]\n",
            "(478, 200)\n",
            "[-0.0093089]\n",
            "(478, 201)\n",
            "[-0.01423264]\n",
            "(478, 202)\n",
            "[-0.00627896]\n",
            "(478, 203)\n",
            "[0.01793995]\n",
            "(478, 204)\n",
            "[-0.01267589]\n",
            "(478, 205)\n",
            "[-0.037649]\n",
            "(478, 206)\n",
            "EJECT at: 222\n",
            "(478, 206)\n",
            "[-0.007961]\n",
            "(478, 207)\n",
            "[-0.0085796]\n",
            "(478, 208)\n",
            "[-0.02759463]\n",
            "(478, 209)\n",
            "padding avoided, SKIP at 226\n",
            "(478, 209)\n",
            "[0.00459054]\n",
            "(478, 210)\n",
            "[0.00937456]\n",
            "(478, 211)\n",
            "[0.03239299]\n",
            "(478, 212)\n",
            "[0.00616022]\n",
            "(478, 213)\n",
            "[0.01468379]\n",
            "(478, 214)\n",
            "[0.01446239]\n",
            "(478, 215)\n",
            "[0.00521187]\n",
            "(478, 216)\n",
            "[-0.00826244]\n",
            "(478, 217)\n",
            "[-0.00990088]\n",
            "(478, 218)\n",
            "[-0.00351054]\n",
            "(478, 219)\n",
            "[0.00234532]\n",
            "(478, 220)\n",
            "[-0.00143061]\n",
            "(478, 221)\n",
            "[0.00821919]\n",
            "(478, 222)\n",
            "[0.00605242]\n",
            "(478, 223)\n",
            "[0.02686779]\n",
            "(478, 224)\n",
            "[-0.00746433]\n",
            "(478, 225)\n",
            "[0.00417506]\n",
            "(478, 226)\n",
            "[0.02807664]\n",
            "(478, 227)\n",
            "[0.0008921]\n",
            "(478, 228)\n",
            "[0.0135213]\n",
            "(478, 229)\n",
            "[-0.03393807]\n",
            "(478, 230)\n",
            "[0.01517553]\n",
            "(478, 231)\n",
            "[0.02748789]\n",
            "(478, 232)\n",
            "[0.01321731]\n",
            "(478, 233)\n",
            "[-0.02419752]\n",
            "(478, 234)\n",
            "[0.00763465]\n",
            "(478, 235)\n",
            "[0.00277224]\n",
            "(478, 236)\n",
            "[0.02283813]\n",
            "(478, 237)\n",
            "[0.00495173]\n",
            "(478, 238)\n",
            "[0.04902406]\n",
            "(478, 239)\n",
            "[-0.00240583]\n",
            "(478, 240)\n",
            "[-0.01356038]\n",
            "(478, 241)\n",
            "[0.00671898]\n",
            "(478, 242)\n",
            "[0.00343923]\n",
            "(478, 243)\n",
            "[-0.00258644]\n",
            "(478, 244)\n",
            "[0.00955247]\n",
            "(478, 245)\n",
            "[0.01909497]\n",
            "(478, 246)\n",
            "[0.00101912]\n",
            "(478, 247)\n",
            "[0.00383274]\n",
            "(478, 248)\n",
            "[-0.00065325]\n",
            "(478, 249)\n",
            "[-0.00470798]\n",
            "(478, 250)\n",
            "[0.00347054]\n",
            "(478, 251)\n",
            "[-0.01671279]\n",
            "(478, 252)\n",
            "[-0.00662242]\n",
            "(478, 253)\n",
            "[0.00851058]\n",
            "(478, 254)\n",
            "[0.01168375]\n",
            "(478, 255)\n",
            "[-0.01224481]\n",
            "(478, 256)\n",
            "[-0.00012972]\n",
            "(478, 257)\n",
            "[-0.01590097]\n",
            "(478, 258)\n",
            "[-0.02888632]\n",
            "(478, 259)\n",
            "EJECT at: 277\n",
            "(478, 259)\n",
            "[0.00462107]\n",
            "(478, 260)\n",
            "[0.01701183]\n",
            "(478, 261)\n",
            "[-0.00501466]\n",
            "(478, 262)\n",
            "[0.03685997]\n",
            "(478, 263)\n",
            "EJECT at: 282\n",
            "(478, 263)\n",
            "[0.00543223]\n",
            "(478, 264)\n",
            "[-0.00462076]\n",
            "(478, 265)\n",
            "[0.00507867]\n",
            "(478, 266)\n",
            "[0.00838187]\n",
            "(478, 267)\n",
            "[0.01162573]\n",
            "(478, 268)\n",
            "[0.00436571]\n",
            "(478, 269)\n",
            "[0.00026731]\n",
            "(478, 270)\n",
            "[0.0112156]\n",
            "(478, 271)\n",
            "[-0.01441212]\n",
            "(478, 272)\n",
            "[-0.03724812]\n",
            "(478, 273)\n",
            "[0.01668597]\n",
            "(478, 274)\n",
            "[0.02128605]\n",
            "(478, 275)\n",
            "[0.02018697]\n",
            "(478, 276)\n",
            "[0.01305973]\n",
            "(478, 277)\n",
            "[0.01111097]\n",
            "(478, 278)\n",
            "[0.00433697]\n",
            "(478, 279)\n",
            "[0.01257331]\n",
            "(478, 280)\n",
            "[0.00749057]\n",
            "(478, 281)\n",
            "[0.00237078]\n",
            "(478, 282)\n",
            "[-0.00421438]\n",
            "(478, 283)\n",
            "[-0.01408788]\n",
            "(478, 284)\n",
            "[0.02080021]\n",
            "(478, 285)\n",
            "[-0.00142291]\n",
            "(478, 286)\n",
            "[0.00637721]\n",
            "(478, 287)\n",
            "[0.00868871]\n",
            "(478, 288)\n",
            "[-0.00537123]\n",
            "(478, 289)\n",
            "EJECT at: 309\n",
            "(478, 289)\n",
            "[0.01504262]\n",
            "(478, 290)\n",
            "[0.02999774]\n",
            "(478, 291)\n",
            "[0.00465389]\n",
            "(478, 292)\n",
            "[-0.00110449]\n",
            "(478, 293)\n",
            "[-0.00311589]\n",
            "(478, 294)\n",
            "[-0.00960604]\n",
            "(478, 295)\n",
            "[0.00400847]\n",
            "(478, 296)\n",
            "EJECT at: 317\n",
            "(478, 296)\n",
            "[0.00653803]\n",
            "(478, 297)\n",
            "[0.01868203]\n",
            "(478, 298)\n",
            "[0.00268249]\n",
            "(478, 299)\n",
            "[-0.00706805]\n",
            "(478, 300)\n",
            "[0.00069587]\n",
            "(478, 301)\n",
            "[0.01198863]\n",
            "(478, 302)\n",
            "[0.02853261]\n",
            "(478, 303)\n",
            "[0.01100509]\n",
            "(478, 304)\n",
            "[0.00364266]\n",
            "(478, 305)\n",
            "[0.02699305]\n",
            "(478, 306)\n",
            "[0.01979408]\n",
            "(478, 307)\n",
            "[0.00600512]\n",
            "(478, 308)\n",
            "padding avoided, SKIP at 330\n",
            "(478, 308)\n",
            "[-0.00838568]\n",
            "(478, 309)\n",
            "[0.00365181]\n",
            "(478, 310)\n",
            "[0.00595248]\n",
            "(478, 311)\n",
            "[-0.02121345]\n",
            "(478, 312)\n",
            "[-0.01827612]\n",
            "(478, 313)\n",
            "[-0.00015746]\n",
            "(478, 314)\n",
            "padding avoided, SKIP at 337\n",
            "(478, 314)\n",
            "[0.02716128]\n",
            "(478, 315)\n",
            "[-0.0095684]\n",
            "(478, 316)\n",
            "[6.8012288e-05]\n",
            "(478, 317)\n",
            "[0.00593228]\n",
            "(478, 318)\n",
            "[0.00461917]\n",
            "(478, 319)\n",
            "[0.00745997]\n",
            "(478, 320)\n",
            "[-0.00069213]\n",
            "(478, 321)\n",
            "[0.01307877]\n",
            "(478, 322)\n",
            "[0.06581386]\n",
            "(478, 323)\n",
            "padding avoided, SKIP at 347\n",
            "(478, 323)\n",
            "[0.0137874]\n",
            "(478, 324)\n",
            "[-0.03311807]\n",
            "(478, 325)\n",
            "[0.01628557]\n",
            "(478, 326)\n",
            "[0.02316103]\n",
            "(478, 327)\n",
            "[0.01218847]\n",
            "(478, 328)\n",
            "[0.01917333]\n",
            "(478, 329)\n",
            "[0.00815483]\n",
            "(478, 330)\n",
            "[0.01441104]\n",
            "(478, 331)\n",
            "[0.03940959]\n",
            "(478, 332)\n",
            "[0.00811627]\n",
            "(478, 333)\n",
            "[-0.00425545]\n",
            "(478, 334)\n",
            "[-0.00262572]\n",
            "(478, 335)\n",
            "[0.0304426]\n",
            "(478, 336)\n",
            "[-0.00278732]\n",
            "(478, 337)\n",
            "[0.00740943]\n",
            "(478, 338)\n",
            "[-0.01618344]\n",
            "(478, 339)\n",
            "[-0.00124528]\n",
            "(478, 340)\n",
            "[0.00668304]\n",
            "(478, 341)\n",
            "[-0.00917855]\n",
            "(478, 342)\n",
            "[0.01977789]\n",
            "(478, 343)\n",
            "[-0.00020763]\n",
            "(478, 344)\n",
            "[-0.03089236]\n",
            "(478, 345)\n",
            "[0.00161013]\n",
            "(478, 346)\n",
            "[0.00228771]\n",
            "(478, 347)\n",
            "padding avoided, SKIP at 372\n",
            "(478, 347)\n",
            "[0.00213118]\n",
            "(478, 348)\n",
            "[-0.00121354]\n",
            "(478, 349)\n",
            "[-0.00054197]\n",
            "(478, 350)\n",
            "[-0.00187469]\n",
            "(478, 351)\n",
            "[0.00034636]\n",
            "(478, 352)\n",
            "[-0.0050981]\n",
            "(478, 353)\n",
            "[0.00218105]\n",
            "(478, 354)\n",
            "[0.02697036]\n",
            "(478, 355)\n",
            "[0.00172705]\n",
            "(478, 356)\n",
            "[-0.00711379]\n",
            "(478, 357)\n",
            "[0.01134981]\n",
            "(478, 358)\n",
            "[0.00790006]\n",
            "(478, 359)\n",
            "[0.00353882]\n",
            "(478, 360)\n",
            "[-0.00613817]\n",
            "(478, 361)\n",
            "[0.01057852]\n",
            "(478, 362)\n",
            "[0.01084377]\n",
            "(478, 363)\n",
            "[-0.00264931]\n",
            "(478, 364)\n",
            "EJECT at: 390\n",
            "(478, 364)\n",
            "[-0.00704616]\n",
            "(478, 365)\n",
            "[0.0292211]\n",
            "(478, 366)\n",
            "[0.00172701]\n",
            "(478, 367)\n",
            "[0.00480623]\n",
            "(478, 368)\n",
            "[0.01040568]\n",
            "(478, 369)\n",
            "[-0.00178866]\n",
            "(478, 370)\n",
            "[0.0146212]\n",
            "(478, 371)\n",
            "padding avoided, SKIP at 398\n",
            "(478, 371)\n",
            "[0.00583371]\n",
            "(478, 372)\n",
            "[-0.0006212]\n",
            "(478, 373)\n",
            "[0.02658241]\n",
            "(478, 374)\n",
            "[0.0160602]\n",
            "(478, 375)\n",
            "[0.00842828]\n",
            "(478, 376)\n",
            "[0.01251317]\n",
            "(478, 377)\n",
            "padding avoided, SKIP at 405\n",
            "(478, 377)\n",
            "[0.02603562]\n",
            "(478, 378)\n",
            "[-0.0018635]\n",
            "(478, 379)\n",
            "[0.00080935]\n",
            "(478, 380)\n",
            "[0.0264831]\n",
            "(478, 381)\n",
            "[0.0040125]\n",
            "(478, 382)\n",
            "[-0.00463988]\n",
            "(478, 383)\n",
            "[0.01523939]\n",
            "(478, 384)\n",
            "[0.00308028]\n",
            "(478, 385)\n",
            "padding avoided, SKIP at 414\n",
            "(478, 385)\n",
            "[-0.00572387]\n",
            "(478, 386)\n",
            "[-0.02081767]\n",
            "(478, 387)\n",
            "[0.0036793]\n",
            "(478, 388)\n",
            "[0.01874009]\n",
            "(478, 389)\n",
            "[0.00734076]\n",
            "(478, 390)\n",
            "[-0.01356477]\n",
            "(478, 391)\n",
            "[0.00563641]\n",
            "(478, 392)\n",
            "[0.01264252]\n",
            "(478, 393)\n",
            "[0.00122164]\n",
            "(478, 394)\n",
            "padding avoided, SKIP at 424\n",
            "(478, 394)\n",
            "[0.00669051]\n",
            "(478, 395)\n",
            "[0.01197255]\n",
            "(478, 396)\n",
            "[0.00214869]\n",
            "(478, 397)\n",
            "[0.01367874]\n",
            "(478, 398)\n",
            "[0.00885897]\n",
            "(478, 399)\n",
            "[0.00022107]\n",
            "(478, 400)\n",
            "[-0.00680194]\n",
            "(478, 401)\n",
            "[0.01689037]\n",
            "(478, 402)\n",
            "[0.00721904]\n",
            "(478, 403)\n",
            "[0.0270911]\n",
            "(478, 404)\n",
            "[0.00575914]\n",
            "(478, 405)\n",
            "[0.00613163]\n",
            "(478, 406)\n",
            "[-0.01075938]\n",
            "(478, 407)\n",
            "[0.02774011]\n",
            "(478, 408)\n",
            "[-0.00419231]\n",
            "(478, 409)\n",
            "[-0.0090056]\n",
            "(478, 410)\n",
            "[0.0031821]\n",
            "(478, 411)\n",
            "[0.00429329]\n",
            "(478, 412)\n",
            "[0.01834383]\n",
            "(478, 413)\n",
            "[0.00563749]\n",
            "(478, 414)\n",
            "padding avoided, SKIP at 445\n",
            "(478, 414)\n",
            "[-0.01799457]\n",
            "(478, 415)\n",
            "EJECT at: 447\n",
            "(478, 415)\n",
            "EJECT at: 448\n",
            "(478, 415)\n",
            "[0.00753164]\n",
            "(478, 416)\n",
            "[-0.01777467]\n",
            "(478, 417)\n",
            "[0.00575213]\n",
            "(478, 418)\n",
            "padding avoided, SKIP at 452\n",
            "(478, 418)\n",
            "padding avoided, SKIP at 453\n",
            "(478, 418)\n",
            "[-0.01128741]\n",
            "(478, 419)\n",
            "[0.00980943]\n",
            "(478, 420)\n",
            "[0.00735683]\n",
            "(478, 421)\n",
            "[0.0674563]\n",
            "(478, 422)\n",
            "[0.04590822]\n",
            "(478, 423)\n",
            "[0.0307802]\n",
            "(478, 424)\n",
            "[0.00559715]\n",
            "(478, 425)\n",
            "[-0.00652748]\n",
            "(478, 426)\n",
            "[0.01049009]\n",
            "(478, 427)\n",
            "[0.02215584]\n",
            "(478, 428)\n",
            "[0.00927527]\n",
            "(478, 429)\n",
            "[0.01466906]\n",
            "(478, 430)\n",
            "[-0.01911769]\n",
            "(478, 431)\n",
            "padding avoided, SKIP at 467\n",
            "(478, 431)\n",
            "[-0.00860014]\n",
            "(478, 432)\n",
            "[0.00528013]\n",
            "(478, 433)\n",
            "[0.00297346]\n",
            "(478, 434)\n",
            "[0.00602401]\n",
            "(478, 435)\n",
            "[0.01306294]\n",
            "(478, 436)\n",
            "[-0.00292174]\n",
            "(478, 437)\n",
            "[-0.02054917]\n",
            "(478, 438)\n",
            "[-0.00588587]\n",
            "(478, 439)\n",
            "[-0.03975644]\n",
            "(478, 440)\n",
            "[0.00995535]\n",
            "(478, 441)\n",
            "[-0.00284534]\n",
            "(478, 442)\n",
            "[0.01835133]\n",
            "(478, 443)\n",
            "[0.00872317]\n",
            "(478, 444)\n",
            "[0.00747179]\n",
            "(478, 445)\n",
            "[0.01451818]\n",
            "(478, 446)\n",
            "[0.01062907]\n",
            "(478, 447)\n",
            "[-0.00349687]\n",
            "(478, 448)\n",
            "[0.00769354]\n",
            "(478, 449)\n",
            "[0.00171799]\n",
            "(478, 450)\n",
            "[0.01585159]\n",
            "(478, 451)\n",
            "[-0.00052418]\n",
            "(478, 452)\n",
            "[0.01066307]\n",
            "(478, 453)\n",
            "[0.00878197]\n",
            "(478, 454)\n",
            "[0.00505973]\n",
            "(478, 455)\n",
            "[0.02761735]\n",
            "(478, 456)\n",
            "[0.0187089]\n",
            "(478, 457)\n",
            "EJECT at: 494\n",
            "(478, 457)\n",
            "[-0.0108338]\n",
            "(478, 458)\n",
            "[-0.00669316]\n",
            "(478, 459)\n",
            "[-0.00136195]\n",
            "(478, 460)\n",
            "[0.0200353]\n",
            "(478, 461)\n",
            "EJECT at: 499\n",
            "(478, 461)\n",
            "[0.01219346]\n",
            "(478, 462)\n",
            "[-0.00085775]\n",
            "(478, 463)\n",
            "[0.00693217]\n",
            "(478, 464)\n",
            "[-0.00118335]\n",
            "(478, 465)\n",
            "[0.004598]\n",
            "(478, 466)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi4D7Yu5scO-",
        "colab_type": "text"
      },
      "source": [
        "# Data optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHpWNAREql4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ERASING FIRST ROW \n",
        "clean_ret = np.delete(ret_cost, 0, 0)\n",
        "clean_ret = np.delete(clean_ret, obj=clean_ret.shape[0]-1, axis=0)\n",
        "\n",
        "#LAST ROW FOR FORECAST\n",
        "to_forecast = ret_cost[477]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r01ERFAuO0Q",
        "colab_type": "code",
        "outputId": "f51ddb49-ab04-4200-9b3b-c39c01ce11ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y = web.get_data_yahoo('^GSPC', start = start, end = today)\n",
        "Y = np.array(Y['Adj Close'].ffill().pct_change())\n",
        "Y = Y.reshape(Y.shape[0],1)\n",
        "Y = np.delete(Y, 0, 0)\n",
        "Y = np.delete(Y, 0, 0)\n",
        "print(clean_ret.shape, Y. shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(476, 466) (476, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFyYbsw0dnOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OYg3gJdTn0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savetxt(\"DATA.csv\", clean_ret, delimiter=\",\")\n",
        "np.savetxt(\"S&P.csv\", Y, delimiter=\",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agyqfuQYeBeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_ret = pd.read_csv(\"/content/drive/My Drive/colab_notebooks/DATA.csv\") \n",
        "Y = pd.read_csv(\"/content/drive/My Drive/colab_notebooks/S&P.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxxT6pNQKOWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rating = np.empty(len(Y), dtype=int)\n",
        "\n",
        "for i in range(0,len(Y)):\n",
        "  if Y[i] >= 0.02 :\n",
        "    rating[i] = 4\n",
        "  elif Y[i] < 0.02 and Y[i] >= 0 :\n",
        "    rating[i] = 3\n",
        "  elif Y[i] < 0 and Y[i] > -0.02 :\n",
        "    rating[i] = 2\n",
        "  elif Y[i]<-0.02 :\n",
        "    rating[i] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JESlGtp7wdl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test, rating_train, rating_test = train_test_split(clean_ret, Y, rating, test_size=0.3, stratify=rating)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFqSlcbuf2FA",
        "colab_type": "code",
        "outputId": "baa5d959-067d-4292-b87a-e77d31dc1480",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "days = x_train.shape[0]\n",
        "ones = np.ones((days, 1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3,\n",
              "       3, 3, 1, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2,\n",
              "       3, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 1, 3, 3, 3, 3, 3, 3, 2,\n",
              "       3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 3, 3, 1, 3, 2, 3, 3, 3, 3, 3, 3, 2,\n",
              "       2, 3, 3, 3, 2, 2, 1, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2,\n",
              "       2, 3, 3, 3, 3, 2, 3, 1, 2, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3,\n",
              "       1, 2, 2, 4, 3, 3, 2, 2, 4, 3, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEuZ5cEMKKaJ",
        "colab_type": "text"
      },
      "source": [
        "#Classification & Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cluw7O0Atjgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.linalg import inv\n",
        "import statistics\n",
        "from keras import Sequential, optimizers\n",
        "from keras.layers import Dense, Conv1D, Conv2D, MaxPooling2D, Flatten, Dropout\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D5A2NfFifS6",
        "colab_type": "text"
      },
      "source": [
        "### *BETA* regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZaXMFNNM5kx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now estimate the regression parameters from the data\n",
        "returns = np.hstack((np.ones((x_train.shape[0], 1)), x_train))\n",
        "\n",
        "S = np.dot(returns.T, returns)\n",
        "S_inv = np.linalg.inv(S)\n",
        "theta_hat = S_inv.dot(returns.T).dot(y_train)\n",
        "\n",
        "print('Estimated theta: {}'.format(theta_hat))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuUdkvJtyVjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reg_test = np.dot(np.hstack((np.ones((x_test.shape[0], 1)), x_test)),theta_hat)\n",
        "regr_rati = np.empty(len(reg_test), dtype=int)\n",
        "for i in range(0,len(reg_test)):\n",
        "  if reg_test[i] >= 0.02 :\n",
        "    regr_rati[i] = 4\n",
        "  elif reg_test[i] < 0.02 and reg_test[i] >= 0 :\n",
        "    regr_rati[i] = 3\n",
        "  elif reg_test[i] < 0 and reg_test[i] > -0.02 :\n",
        "    regr_rati[i] = 2\n",
        "  elif reg_test[i]<-0.02 :\n",
        "    regr_rati[i] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oR-iS2C1F6L",
        "colab_type": "code",
        "outputId": "c7efa89f-6686-4d1b-ea5c-eb9948ca8c61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.mean(np.subtract(rating_test,regr_rati))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.02097902097902098"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqJKmUog62Ey",
        "colab_type": "code",
        "outputId": "1b3a35f9-847a-4a6f-ecd4-c35fbdf2bac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(np.mean(np.subtract(x_test,reg_test)),np.max(reg_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.21187147341350707 4.953172686428929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuFvF3LyyVL8",
        "colab_type": "text"
      },
      "source": [
        "# Advanced Netowrks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIC8BIdjKOPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standardize data\n",
        "X = (clean_ret - clean_ret.mean(0)) / clean_ret.std(0)\n",
        "\n",
        "legendary = np.empty(len(rating), dtype=int)\n",
        "for i in range(0,len(rating)):\n",
        "  if rating[i] >= 4 :\n",
        "    legendary[i] = 1\n",
        "  else :\n",
        "    legendary[i] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49LIBKEGKOBw",
        "colab_type": "code",
        "outputId": "1d2064f7-c200-4a5c-c728-8072cfdf3507",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Split train / test / validation data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, legendary, test_size=0.1, stratify=legendary)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train)\n",
        "\n",
        "# Compute class weights\n",
        "days = X_train.shape[0]\n",
        "n_legendaries = legendary.sum()\n",
        "n_classes = 2\n",
        "class_weights = {0: days / (n_classes * (days - n_legendaries)), #common weight\n",
        "                 1: days / (n_classes * n_legendaries)}          #legendary weight\n",
        "\n",
        "print('Training data: {} legendaries out of {} days'.format(n_legendaries, days))\n",
        "print('Training data: class weights {}'.format(class_weights))\n",
        "print(X_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data: 116 legendaries out of 352 days\n",
            "Training data: class weights {0: 0.7457627118644068, 1: 1.5172413793103448}\n",
            "(352, 505)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_4rr9IR4GF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network=Sequential()\n",
        "network.add(Dense(32, activation='relu', input_shape=X.shape[1:]))\n",
        "network.add(Dense(10, activation='relu'))\n",
        "network.add(Dense(5, activation='relu'))\n",
        "network.add(Dense(1, activation='softmax'))\n",
        "network.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'], weighted_metrics=['accuracy'] )\n",
        "network.fit(X_train, y_train, epochs=100,validation_data=[X_val,y_val] ,class_weight=class_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSRfr1n66345",
        "colab_type": "code",
        "outputId": "02d7bf73-0a07-4d36-ce07-6801865ba554",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "train_scores = network.evaluate(X_train,y_train, verbose=0)\n",
        "val_scores = network.evaluate(X_val,y_val, verbose=0)\n",
        "print('Test loss: {} - Test acc: {}'.format(*train_scores))\n",
        "print('Validation loss: {} - Validation Acc:{}'.format(*val_scores))\n",
        "pd.DataFrame(network.history.history).plot()\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0,1)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 11.730334715409713 - Test acc: 0.26420454545454547\n",
            "Validation loss: 11.558229160308837 - Validation Acc:0.275\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD1/QOwwtAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s4vKyWctfLg",
        "colab_type": "text"
      },
      "source": [
        "# NN Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI12csvrxF1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(clean_ret, Y, test_size=0.2, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUMUTi4bmTmy",
        "colab_type": "code",
        "outputId": "72cce49b-7527-41ef-fbeb-47456a80cdb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(348, 505)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJwvuhJK74zK",
        "colab_type": "text"
      },
      "source": [
        "## Basic Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7dr5RVkwbaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build model\n",
        "model = Sequential()\n",
        "model.add(Dense(300, activation='relu'))\n",
        "model.add(Dense(150, activation='relu'))\n",
        "model.add(Dense(1,activation='relu'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qDTbmsYwbNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(x_train,y_train, epochs=30, validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4M8s00-7yQM",
        "colab_type": "text"
      },
      "source": [
        "## Convolutional model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaPC9Nrcvio8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(50, (3, 3), activation='tanh', input_shape= (,505) ) )\n",
        "model.add(Conv2D(100, (3, 3), activation='tanh'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(300, activation='tanh'))\n",
        "model.add(Dense(128, activation='tanh'))\n",
        "model.add(Dense(50, activation='tanh'))\n",
        "model.add(Dense(1, activation='tanh'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9PlO_WUpvei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='sgd', loss='mean_squared_error', metrics=['accuracy'])\n",
        "model.fit(x_train,y_train, epochs=30, validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnCnRHMMqAYP",
        "colab_type": "text"
      },
      "source": [
        "##Test NNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t66GToXMZQ34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = keras.layers.Input(shape=x_train.shape)\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = keras.layers.Concatenate()([input, hidden2])\n",
        "output = keras.layers.Dense(1)(concat)\n",
        "model = keras.models.Model(inputs=[input], outputs=[output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzzCOiWefQxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " model.compile(loss=\"mse\",\n",
        "                  optimizer=\"sgd\",\n",
        "                  metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g49g9YFNgE8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(x_train,y_train, epochs=30, validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn9gjgrFUbus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = keras.models.Sequential()\n",
        "net.add(keras.layers.Flatten())\n",
        "net.add(keras.layers.Dense(300, activation=\"tanh\"))\n",
        "net.add(keras.layers.Dense(100, activation=\"tanh\"))\n",
        "net.add(keras.layers.Dense(10, activation=\"softmax\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcmXyYYOhWtG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net.compile(loss=\"mean_squared_error\", optimizer=\"sgd\", metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRahiM82hZO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net.fit(x_train,y_train, epochs=200, validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}